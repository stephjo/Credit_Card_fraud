{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will use various predictive models to see how accurate they are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main challenge when it comes to modeling fraud detection as a classification problem comes from the fact that in real world data, the majority of transactions is not fraudulent. Investment in technology for fraud detection has increased over the years so this shouldn’t be a surprise, but this brings us a problem: imbalanced data.\n",
    "There are many ways of dealing with imbalanced data. We will focus in the following approaches:<br/>\n",
    "1.Oversampling — RandomOverSampler<br/>\n",
    "2.Oversampling — SMOTE<br/>\n",
    "2.Undersampling — RandomUnderSampler<br/>\n",
    "3.Combined Class Methods — SMOTE+ENN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Understand the data</b>\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "    \n",
    "The first thing we must do is gather a basic sense of our data. Remember, except for the transaction and amount we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.<br/><br/>\n",
    "The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).Keep in mind that in order to implement a PCA transformation features need to be previously scaled, i.e all the V features have been scaled.<br/><br/>\n",
    "Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "import collections\n",
    "# Other Libraries\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import auc,precision_score, recall_score, f1_score, roc_auc_score, accuracy_score,precision_recall_curve, classification_report,confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:/Users/StephyJosin/Desktop/Projects/Credit_card\")\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are heavily skewed and we will fix this later.\n",
    "# print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "# print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
    "print('No Frauds', round(df['Class'].value_counts(normalize=True)[0] * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts(normalize=True)[1]*100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Class', data=df, palette=[\"#0101DF\", \"#DF0101\"])\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amount and Time feature distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = df['Amount'].values\n",
    "time_val = df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to scale the amount and time features as all other V features has already been scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "#create new scaled columns\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "#drop original columns\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "#plot amount and time after scaling\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "ax[0].set_title('Amount After Robust Scaling')\n",
    "sns.kdeplot(df['scaled_amount'], ax=ax[0])\n",
    "ax[1].set_title('Time After Robust Scaling')\n",
    "sns.kdeplot(df['scaled_time'], ax=ax[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move new scaled columns to the front\n",
    "cols_at_end = ['scaled_amount', 'scaled_time']\n",
    "df = df[[c for c in cols_at_end if c in df]+[c for c in df if c not in cols_at_end]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Splitting the data</b><br/><br/>\n",
    "Before proceeding with resampling techniques ,we are going to split the dataset into training and test set.Then will apply resampling techniques on the training data and build our models on the resampled training data.But we will be testing our models on the original test data.<br/>\n",
    "If the number of values belonging to each class are unbalanced, using <b>stratified split</b> is a good thing. You are basically asking the model to take the training and test set such that the class proportion is same as of the whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "print('No Frauds', round(df['Class'].value_counts(normalize=True)[0] * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts(normalize=True)[1]*100,2), '% of the dataset')\n",
    "\n",
    "#drop the target column from training data\n",
    "X_features = df.drop('Class', axis=1)#training features\n",
    "y_target= df['Class']#target column\n",
    "\n",
    "#PCA transformed amount and time features\n",
    "#X = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
    "print(X_features.head())\n",
    "print(y_target.head())\n",
    "#Stratification is done based on the y labels.\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(X_features, y_target):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_trainX, original_testX = X_features.iloc[train_index], X_features.iloc[test_index]\n",
    "    original_train_y, original_test_y = y_target.iloc[train_index], y_target.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Resampling Techniques</h1>\n",
    "\n",
    "<h2>1.Random Under-Sampling</h2><br/>\n",
    "Random Undersampling aims to balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out.<br/>\n",
    "\n",
    "<b>Advantages</b><br/>\n",
    "It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.<br/>\n",
    "<b>Disadvantages</b><br/>\n",
    "It can discard potentially useful information which could be important for building rule classifiers.<br/>\n",
    "The sample chosen by random under sampling may be a biased sample. And it will not be an accurate representative of the<br/> population. Thereby, resulting in inaccurate results with the actual test data set.<br/>\n",
    "\n",
    "<h2>2.Random Over-Sampling</h2><br/>\n",
    "Over-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.<br/>\n",
    "<b>Advantages</b><br/>\n",
    "Unlike under sampling this method leads to no information loss.\n",
    "Outperforms under sampling<br/>\n",
    "<b>Disadvantages</b><br/>\n",
    "It increases the likelihood of overfitting since it replicates the minority class events.<br/>\n",
    "\n",
    "<h2>3.SMOTE — Synthetic Minority Over-sampling Technique</h2><br/>\n",
    "SMOTE creates synthetic observations of the minority class (in this case, fraudulent transactions). At a lower level, SMOTE performs the following steps:<br/><br/>\n",
    "\n",
    "Finding the k-nearest-neighbors for minority class observations (finding similar observations)<br/>\n",
    "Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations.<br/>\n",
    "\n",
    "<b>Advantages</b><br/>\n",
    "Mitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances<br/>\n",
    "No loss of useful information<br/>\n",
    "<b>Disadvantages</b><br/>\n",
    "While generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise<br/>\n",
    "SMOTE is not very effective for high dimensional data<br/>\n",
    "\n",
    "<h2>4.Combined class methods-SMOTEENN</h2><br/>\n",
    "SMOTE can generate noisy samples by interpolating new points between marginal outliers and inliers. This issue can be solved by cleaning the resulted space obtained after over-sampling.<br/>\n",
    "\n",
    "In this regard, we will use SMOTE together with edited nearest-neighbours (ENN). Here, ENN is used as the cleaning method after SMOTE over-sampling to obtain a cleaner space. This is something that is easily achievable by using imblearn’s SMOTEENN class.SMOTEENN is a combination of over- and under-sampling methods<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model,X_train,Y_train,X_Test,Y_Test,sampler): \n",
    "    '''\n",
    "    This function resamples the training data,trains the model with resampled training data,\n",
    "    predict the output of test data with the trained model\n",
    "    - model=machinelearning model\n",
    "    - X_train=training features\n",
    "    - Y_train=training target\n",
    "    - X_test=testdata features\n",
    "    - Y_test=testdata target\n",
    "    - sampler=sampler object\n",
    "    '''\n",
    "    x_train_sampled, y_train_sampled = sampler.fit_sample(X_train,Y_train)\n",
    "    model.fit(x_train_sampled, y_train_sampled)\n",
    "    #predicting test data outputs\n",
    "    predictions = model.predict(X_Test)\n",
    "    eval_metrics(model,Y_Test, predictions)\n",
    "    \n",
    "def plot_pr_curve(model_auc_pr,y_pred,y_test):\n",
    "    '''\n",
    "    This function plot the area under precision recall curve\n",
    "    -model_auc_pr = precision_recall area under curve\n",
    "    '''\n",
    "    # calculate precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(testy, probs)\n",
    "    # calculate F1 score\n",
    "    f1 = f1_score(testy, yhat)\n",
    "    # calculate precision-recall AUC\n",
    "    auc = auc(recall, precision)\n",
    "    # calculate average precision score\n",
    "    ap = average_precision_score(testy, probs)\n",
    "    print('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))\n",
    "    # plot no skill\n",
    "    plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    recall, precision, thresholds = precision_recall_curve(y_test,y_pred, pos_label=1)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=' (area = %0.2f)' % model_auc_pr)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('PRC')\n",
    "    plt.show()\n",
    "\n",
    "def eval_metrics(model,Y_Test, predictions):  \n",
    "    '''\n",
    "    This function calculates the evaluation metrics\n",
    "    model-model object\n",
    "    Y_Test-actual target values\n",
    "    predictions-predcicted target values\n",
    "    \n",
    "    '''\n",
    "    #Accuracy score\n",
    "    print('Accuracy_score')\n",
    "    print('-'*40)\n",
    "    print(accuracy_score(Y_Test, predictions))\n",
    "    #Confusion matrix \n",
    "    print('\\n Confusion_matrix')\n",
    "    print('-'*40)\n",
    "    print(confusion_matrix(Y_Test, predictions))\n",
    "    #Classification Report\n",
    "    print('\\n Classification Report')\n",
    "    print('-'*40)\n",
    "    print(classification_report(Y_Test, predictions))\n",
    "    #pr_auc_score\n",
    "    model_auc_pr = pr_auc_score(Y_Test,predictions) \n",
    "    print ('\\n Area under curve')\n",
    "    print('-'*40)\n",
    "    print(model_auc_pr)\n",
    "   # plot_pr_curve(model_auc_pr,predictions,Y_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc_score(y_test,y_pred):\n",
    "    '''\n",
    "        This function computes area under the precision-recall curve. \n",
    "    '''\n",
    "      \n",
    "    precisions, recalls,_ = precision_recall_curve(y_test,y_pred, pos_label=1)\n",
    "    \n",
    "    return auc(recalls, precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv=5\n",
    "\n",
    "GBmodel=GradientBoostingClassifier()\n",
    "rfmodel=RandomForestClassifier()\n",
    "\n",
    "#  Random Under Sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "print('Random under-sampling\")\n",
    "train_test(rfmodel,original_trainX,original_train_y, original_testX, original_test_y, RandomUnderSampler())\n",
    "\n",
    "# Random over Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "print('-'*40)\n",
    "print(\"Random over-sampling\")\n",
    "train_test(rfmodel, original_trainX,original_train_y, original_testX, original_test_y, RandomOverSampler())\n",
    "\n",
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print('-'*40)\n",
    "print(\"SMOTE over-sampling\")\n",
    "train_test(rfmodel, original_trainX,original_train_y, original_testX, original_test_y, SMOTE())\n",
    "\n",
    "# SMOTEENN\n",
    "from imblearn.combine import SMOTEENN \n",
    "print('-'*40)\n",
    "print(\"SMOTEENN over-sampling\")\n",
    "train_test(rfmodel, original_trainX,original_train_y, original_testX, original_test_y, SMOTEENN())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "Results shows that this dataset generates better precision recall curve area when applied with random over sampling technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
